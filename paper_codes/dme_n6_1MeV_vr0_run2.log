==========================================
SLURM_JOB_ID = 4905350
SLURM_JOB_NODELIST = d05-29,e06-13
TMPDIR = /tmp/SLURM_4905350
==========================================
[0 : output_mpi] Output to be read-from/written-into folder 'chains', with prefix 'dme_n6_1MeV_vr0_run2'
[0 : output_mpi] Found existing info files with the requested output prefix: 'chains/dme_n6_1MeV_vr0_run2'
[0 : output_mpi] *ERROR* Delete the previous output manually, automatically ('-f', '--force', 'force: True') or request resuming ('-r', '--resume', 'resume: True')
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
slurmstepd: error: *** STEP 4905350.0 ON d05-29 CANCELLED AT 2021-07-03T19:02:34 ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: error: d05-29: task 0: Exited with exit code 1
srun: error: e06-13: task 1: Killed
