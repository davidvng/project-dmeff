==========================================
SLURM_JOB_ID = 4905359
SLURM_JOB_NODELIST = d06-[23-24]
TMPDIR = /tmp/SLURM_4905359
==========================================
[0 : output_mpi] Output to be read-from/written-into folder 'chains', with prefix 'dme_n6_1keV_vr0_run2'
[0 : output_mpi] Found existing info files with the requested output prefix: 'chains/dme_n6_1keV_vr0_run2'
[0 : output_mpi] *ERROR* Delete the previous output manually, automatically ('-f', '--force', 'force: True') or request resuming ('-r', '--resume', 'resume: True')
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 4905359.0 ON d06-23 CANCELLED AT 2021-07-03T19:19:58 ***
srun: error: d06-24: task 1: Killed
srun: error: d06-23: task 0: Exited with exit code 1
